
val df = spark.read.option("samplingRatio",0.2).json("hdfs://hdpmst:9000/data/20news_json")

df.rdd.getNumPartitions
res0: Int = 3

df.printSchema
root
 |-- filename: string (nullable = true)
 |-- text: string (nullable = true)
 
val df1 = df.withColumn("topic", split('filename, "/")(3))

spark.conf.set("spark.sql.shuffle.partitions", 12)

import org.apache.spark.ml.feature.RegexTokenizer
val tokenizer = new RegexTokenizer().setInputCol("text").setOutputCol("words").setPattern("""\W+""")

import org.apache.spark.ml.feature.SQLTransformer

val sql_explode = new SQLTransformer().setStatement("SELECT EXPLODE(words) AS word FROM __THIS__")

val regex_chknum = (word:String) => word.matches(".*\\d+.*")
spark.udf.register("regex_chknum",regex_chknum)

val sql = new SQLTransformer().setStatement("""
-- get tokens with one-character length
SELECT word FROM __THIS__ WHERE LENGTH(word) = 1   
UNION
-- remove terms with only one-occurrence
SELECT word FROM __THIS__ GROUP BY word HAVING COUNT(*) = 1  
UNION
-- filter out numbers and tokens that are words mixed with numbers
SELECT word FROM __THIS__ WHERE regex_chknum(word)   
""")

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(tokenizer,sql_explode,sql))

val model = pipeline.fit(df1)

val wholeFilters = model.transform(df1)

wholeFilters.count  // 106328

val removedWords = wholeFilters.select("word").map( x => x.getString(0)).collect.toArray


// remove stop words
import org.apache.spark.ml.feature.StopWordsRemover
val enStopWords = StopWordsRemover.loadDefaultStopWords("english")
val remover = new StopWordsRemover().setStopWords(enStopWords).
setInputCol("words").
setOutputCol("filteredStopWords")

// remove tokens collected in removedListWords
import org.apache.spark.ml.feature.StopWordsRemover
val remover_sql = new StopWordsRemover().setStopWords(removedWords).
setInputCol("filteredStopWords").
setOutputCol("filtered")

val dim = math.pow(2, 17).toInt  // 131072

import org.apache.spark.ml.feature.HashingTF
val tf = new HashingTF().setInputCol("filtered").
setOutputCol("TFOut").
setNumFeatures(dim)

import org.apache.spark.ml.feature.IDF
val idf = new IDF().setInputCol("TFOut").setOutputCol("features")

import org.apache.spark.ml.classification.{LogisticRegression, OneVsRest}
val lr = new LogisticRegression
lr.setRegParam(0.01).setMaxIter(100).setFitIntercept(true)

val ovr = new OneVsRest().setClassifier(lr)

import org.apache.spark.ml.feature.{StringIndexer}

val resultStrIdx = new StringIndexer().setInputCol("topic").setOutputCol("label")

val pipeline1 = new Pipeline().setStages(Array(resultStrIdx,tokenizer,remover,remover_sql,tf,idf,ovr))

val Array(trainingData, testData) = df1.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
trainingData.count  // 13209

val model1 = pipeline1.fit(trainingData)

val pred = model1.transform(testData)

pred.select('label, 'prediction).show
+-----+----------+                                                              
|label|prediction|
+-----+----------+
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
+-----+----------+
only showing top 20 rows

import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")

evaluator.evaluate(pred)  // 0.6902322346435054

val predRDD = pred.select("prediction","label").rdd.map( row => (row.getDouble(0),row.getDouble(1))).cache

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

predRDD.filter(x => x._1 == x._2).count  // 81377
predRDD.count     // 117898
metrics.accuracy  // 0.6902322346435054

metrics.confusionMatrix


model1.write.save("hdfs://hdpmst:9000/model/news")


---------------

import org.apache.spark.ml.PipelineModel
val model = PipelineModel.load("hdfs://hdpmst:9000/model/news")

val df = spark.read.option("samplingRatio",0.2).json("hdfs://hdpmst:9000/data/20news_json")
 
val df1 = df.withColumn("topic", split('filename, "/")(3))

val Array(trainingData, testData) = df1.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
trainingData.count  // 13269

spark.conf.set("spark.sql.shuffle.partitions",10)

val pred = model.transform(testData)

pred.select('label, 'prediction).show
+-----+----------+                                                              
|label|prediction|
+-----+----------+
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
| 17.0|      17.0|
+-----+----------+
only showing top 20 rows