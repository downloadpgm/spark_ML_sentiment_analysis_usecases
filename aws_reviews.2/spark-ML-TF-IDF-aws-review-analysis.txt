
val reviews = sc.textFile("aws_reviews/test.ft.txt").map(x => Array(x.slice(0,10),x.slice(11,x.size)))

val rdd = reviews.map( x => {
  val result = if (x(0) == "__label__2") "1" else "0"  // 0 = negative, 1 = positive
  Array(x(1),result)
})

import org.apache.spark.sql.types._
val df1 = rdd.map( x => (x(0),x(1))).toDF("text","result").withColumn("label", 'result.cast(DoubleType))

df1.printSchema
root
 |-- text: string (nullable = true)
 |-- result: string (nullable = true)
 |-- label: double (nullable = true)


import org.apache.spark.ml.feature.RegexTokenizer
val tokenizer = new RegexTokenizer().setInputCol("text").setOutputCol("words").setPattern("""\W+""")
val df2 = tokenizer.transform(df1)

df2.select(explode('words).as("word")).distinct.count  // 259143

// filter out numbers and tokens that are words mixed with numbers
val filterNumbers = df2.select(explode('words).as("word")).where('word.rlike("^[0-9]*$")).distinct

// lists tokens with one-character length
val tokenCountsFilteredSize = df2.select(explode('words).as("word")).where(length('word) === 1).distinct

// remove terms with only one-occurrence
val rareTokens = df2.select(explode('words).as("word")).groupBy('word).count.where('count === 1).select('word)

// unioned all terms to be removed
val wholeFilters = filterNumbers.union(tokenCountsFilteredSize).union(rareTokens).distinct.cache

wholeFilters.count  // 142666

wholeFilters.printSchema
root
 |-- word: string (nullable = true)
 
val removedWords= wholeFilters.select("word").map( x => x.getString(0)).collect.toArray
removedWords: Array[String] = Array(07, 1090, 675, 829, 46534, 0787959375, 3210, 296, 691, 9009,467, 29210, 5325, 36067, 4821, overspender, veenjoyed, adjustmnet, whishing, bumpper, flossie, panzy, dhakaite, adapterwhat, ridicilously, schismatrix, yoki, sidedness, yogananada, gloibal, klevit, appropreiate, nicholaa, dropdeadgorgeous, persents, supportall, schoolmasterly, suzhou, prblems, fuzzys, prgramming, bisessualita, ewolfs, batteryit, afavor, walat, potentialspend, willamettes, npo, 80xl, worplesdon, gneat, jacksonian, draggingly, miron, i200, pujol, candy4, himevery, diabolico, spacdey, tonsorial, nickelcreedpuddleofsalivadefault3doorswhatever, bargello, killling, m203, throughagain, nearlt, copelandesque, 2kg, 2008tt, fafchampsrecorded, tdks, fixity, themesmore, recommit, orn, s...

// remove stop words
import org.apache.spark.ml.feature.StopWordsRemover
val enStopWords = StopWordsRemover.loadDefaultStopWords("english")
val remover = new StopWordsRemover().setStopWords(enStopWords).
setInputCol("words").
setOutputCol("filteredStopWords")
val df3 = remover.transform(df2)

// total words after stopwords removal
df3.select(explode('filteredStopWords).as("word")).distinct.count  // 259012

// remove tokens collected in removedListWords
import org.apache.spark.ml.feature.StopWordsRemover
val remover = new StopWordsRemover().setStopWords(removedWords).
setInputCol("filteredStopWords").
setOutputCol("filtered")
val df4 = remover.transform(df3)

// total words relevant for analysis
df4.select(explode('filtered).as("word")).distinct.count  // 116350

val dim = math.pow(2, 17).toInt  // 131072

import org.apache.spark.ml.feature.HashingTF
val tf = new HashingTF().setInputCol("filtered").
setOutputCol("TFOut").
setNumFeatures(dim)
val df5 = tf.transform(df4)

import org.apache.spark.ml.feature.IDF
val idf = new IDF().setInputCol("TFOut").setOutputCol("features")
val idfModel = idf.fit(df5)
val df6 = idfModel.transform(df5)

df6.printSchema
root
 |-- text: string (nullable = true)
 |-- result: string (nullable = true)
 |-- label: double (nullable = true)
 |-- words: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- filteredStopWords: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- filtered: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- TFOut: vector (nullable = true)
 |-- features: vector (nullable = true)


val Array(trainingData, testData) = df6.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
trainingData.count  // 279694

---- ML Logistic classification --------------

import org.apache.spark.ml.classification.{LogisticRegression}
val lr = new LogisticRegression
lr.setRegParam(0.01).setMaxIter(200).setFitIntercept(true)

val model = lr.fit(trainingData)
val pred = model.transform(testData).cache

import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")

evaluator.evaluate(pred)  // 0.864163050886905

val predRDD = pred.select("prediction","label").rdd.map( row => (row.getDouble(0),row.getInt(1).toDouble)).cache

predRDD.take(20)
res7: Array[(Double, Double)] = Array((1.0,1.0), (1.0,1.0), (0.0,0.0), (1.0,1.0), (1.0,1.0), (0.0,0.0), (1.0,1.0), (1.0,1.0), (0.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0),(1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

predRDD.filter(x => x._1 == x._2).count  // 103964
predRDD.count     // 120306
metrics.accuracy  // 0.864163050886905

metrics.confusionMatrix
res11: org.apache.spark.mllib.linalg.Matrix =
51769.0  8401.0
7941.0   52195.0

---- ML Naive Bayes classification --------------

import org.apache.spark.ml.classification.NaiveBayes
val model = new NaiveBayes().fit(trainingData)

val pred = model.transform(testData).cache

import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")

evaluator.evaluate(pred)  // 0.812943660332818

val predRDD = pred.select("prediction","label").rdd.map( row => (row(0).toString.toDouble,row(1).toString.toDouble)).cache

predRDD.take(20)
res13: Array[(Double, Double)] = Array((1.0,1.0), (1.0,1.0), (0.0,0.0), (1.0,1.0), (1.0,1.0), (0.0,0.0), (1.0,1.0), (0.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

predRDD.filter(x => x._1 == x._2).count  // 97802
predRDD.count     // 120306
metrics.accuracy  // 0.812943660332818

metrics.confusionMatrix
res17: org.apache.spark.mllib.linalg.Matrix =
49847.0  10323.0
12181.0  47955.0