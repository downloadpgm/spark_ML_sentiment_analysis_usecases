
val rdd = sc.textFile("hdfs://hdpmst:9000/data/test.ft.txt")

val rdd1 = rdd.map( x => (x.substring(0,10),x.substring(11,x.size)))

rdd1.toDF("result","text").show
+----------+--------------------+
|    result|                text|
+----------+--------------------+
|__label__2|Great CD: My love...|
|__label__2|One of the best g...|
|__label__1|Batteries died wi...|
|__label__2|works fine, but M...|
|__label__2|Great for the non...|
|__label__1|DVD Player crappe...|
|__label__1|Incorrect Disc: I...|
|__label__1|DVD menu select p...|
|__label__2|Unique Weird Orie...|
|__label__1|Not an "ultimate ...|
|__label__2|Great book for tr...|
|__label__1|Not!: If you want...|
|__label__1|A complete Bust: ...|
|__label__2|TRULY MADE A DIFF...|
|__label__1|didn't run off of...|
|__label__1|Don't buy!: First...|
|__label__2|Simple, Durable, ...|
|__label__2|Review of Kelly C...|
|__label__2|SOY UN APASIONADO...|
|__label__2|Some of the best ...|
+----------+--------------------+
only showing top 20 rows

val df = rdd1.toDF("result","text")

df.printSchema
root
 |-- result: string (nullable = true)
 |-- text: string (nullable = true)


df.groupBy('result).count.show
+----------+------+                                                             
|    result| count|
+----------+------+
|__label__2|200000|
|__label__1|200000|
+----------+------+

val df1 = df.withColumn("label",when('result === "__label__1",0.0).otherwise(1.0))
df1: org.apache.spark.sql.DataFrame = [result: string, text: string ... 1 more field]

df1.printSchema
root
 |-- result: string (nullable = true)
 |-- text: string (nullable = true)
 |-- label: double (nullable = false)

spark.conf.set("spark.sql.shuffle.partitions", 12)

import org.apache.spark.ml.feature.RegexTokenizer
val tokenizer = new RegexTokenizer().setInputCol("text").setOutputCol("words").setPattern("""\W+""")

import org.apache.spark.ml.feature.SQLTransformer

val sql_explode = new SQLTransformer().setStatement("SELECT EXPLODE(words) AS word FROM __THIS__")

val regex_chknum = (word:String) => word.matches(".*\\d+.*")
spark.udf.register("regex_chknum",regex_chknum)

val sql = new SQLTransformer().setStatement("""
-- get tokens with one-character length
SELECT word FROM __THIS__ WHERE LENGTH(word) = 1   
UNION
-- remove terms with only one-occurrence
SELECT word FROM __THIS__ GROUP BY word HAVING COUNT(*) = 1  
UNION
-- filter out numbers and tokens that are words mixed with numbers
SELECT word FROM __THIS__ WHERE regex_chknum(word)   
""")

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(tokenizer,sql_explode,sql))

val model = pipeline.fit(df1)

val wholeFilters = model.transform(df1)

wholeFilters.count  // 147032

val removedWords = wholeFilters.select("word").map( x => x.getString(0)).collect.toArray


// remove stop words
import org.apache.spark.ml.feature.StopWordsRemover
val enStopWords = StopWordsRemover.loadDefaultStopWords("english")
val remover = new StopWordsRemover().setStopWords(enStopWords).
setInputCol("words").
setOutputCol("filteredStopWords")

// remove tokens collected in removedListWords
import org.apache.spark.ml.feature.StopWordsRemover
val remover_sql = new StopWordsRemover().setStopWords(removedWords).
setInputCol("filteredStopWords").
setOutputCol("filtered")

val dim = math.pow(2, 17).toInt  // 131072

import org.apache.spark.ml.feature.HashingTF
val tf = new HashingTF().setInputCol("filtered").
setOutputCol("TFOut").
setNumFeatures(dim)

import org.apache.spark.ml.feature.IDF
val idf = new IDF().setInputCol("TFOut").setOutputCol("features")

import org.apache.spark.ml.classification.LogisticRegression
val lr = new LogisticRegression
lr.setRegParam(0.01).setMaxIter(100).setFitIntercept(true)

val pipeline1 = new Pipeline().setStages(Array(tokenizer,remover,remover_sql,tf,idf,lr))

val Array(trainingData, testData) = df1.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
trainingData.count  // 279694

val model1 = pipeline1.fit(trainingData)

val pred = model1.transform(testData)

pred.select('label, 'prediction).show
+-----+----------+                                                              
|label|prediction|
+-----+----------+
|  0.0|       1.0|
|  0.0|       1.0|
|  0.0|       1.0|
|  0.0|       0.0|
|  0.0|       1.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       1.0|
|  0.0|       0.0|
|  0.0|       0.0|
+-----+----------+
only showing top 20 rows

import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")

evaluator.evaluate(pred)  // 0.8626834904327298

val predRDD = pred.select("prediction","label").rdd.map( row => (row.getDouble(0),row.getDouble(1))).cache

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

predRDD.filter(x => x._1 == x._2).count  // 103786
predRDD.count     // 120306
metrics.accuracy  // 0.8626834904327298

metrics.confusionMatrix
res26: org.apache.spark.mllib.linalg.Matrix =                                   
51395.0  8627.0
7893.0   52391.0

model1.write.save("hdfs://hdpmst:9000/model/aws_review")


---------------

import org.apache.spark.ml.PipelineModel
val model = PipelineModel.load("hdfs://hdpmst:9000/model/aws_review")

val rdd = sc.textFile("hdfs://hdpmst:9000/data/test.ft.txt")

val rdd1 = rdd.map( x => (x.substring(0,10),x.substring(11,x.size)))

val df = rdd1.toDF("result","text")

val df1 = df.withColumn("label",when('result === "__label__1",0.0).otherwise(1.0))

val Array(trainingData, testData) = df1.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
trainingData.count  // 279694

spark.conf.set("spark.sql.shuffle.partitions",10)

val pred = model.transform(testData)

pred.select('label, 'prediction).show
+-----+----------+                                                              
|label|prediction|
+-----+----------+
|  0.0|       1.0|
|  0.0|       1.0|
|  0.0|       1.0|
|  0.0|       0.0|
|  0.0|       1.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       1.0|
|  0.0|       0.0|
|  0.0|       0.0|
+-----+----------+
only showing top 20 rows
