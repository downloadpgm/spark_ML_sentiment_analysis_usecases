
val df = spark.read.format("json").load("sarcasm_hdline/Sarcasm_Headlines_Dataset.json")

df.printSchema
root
 |-- article_link: string (nullable = true)
 |-- headline: string (nullable = true)
 |-- is_sarcastic: long (nullable = true)
 
df.show
+--------------------+--------------------+------------+
|        article_link|            headline|is_sarcastic|
+--------------------+--------------------+------------+
|https://www.huffi...|former versace st...|           0|
|https://www.huffi...|the 'roseanne' re...|           0|
|https://local.the...|mom starting to f...|           1|
|https://politics....|boehner just want...|           1|
|https://www.huffi...|j.k. rowling wish...|           0|
|https://www.huffi...|advancing the wor...|           0|
|https://www.huffi...|the fascinating c...|           0|
|https://www.huffi...|this ceo will sen...|           0|
|https://politics....|top snake handler...|           1|
|https://www.huffi...|friday's morning ...|           0|
|https://www.huffi...|airline passenger...|           0|
|https://www.huffi...|facebook reported...|           0|
|https://www.huffi...|north korea prais...|           0|
|https://www.huffi...|actually, cnn's j...|           0|
|https://www.huffi...|barcelona holds h...|           0|
|https://entertain...|nuclear bomb deto...|           1|
|https://www.theon...|cosby lawyer asks...|           1|
|https://www.theon...|stock analysts co...|           1|
|https://www.huffi...|bloomberg's progr...|           0|
|https://www.huffi...|craig hicks indicted|           0|
+--------------------+--------------------+------------+

val df1 = df.select("headline", "is_sarcastic").withColumnRenamed("is_sarcastic","label")

df1.printSchema
root
 |-- headline: string (nullable = true)
 |-- label: long (nullable = true)

import org.apache.spark.ml.feature.RegexTokenizer
val tokenizer = new RegexTokenizer().setInputCol("headline").setOutputCol("words").setPattern("""\W+""")
val df2 = tokenizer.transform(df1)

df2.select(explode('words).as("word")).distinct.count  // 25319

// filter out numbers and tokens that are words mixed with numbers
val filterNumbers = df2.select(explode('words).as("word")).where('word.rlike("^[0-9]*$")).distinct

// lists tokens greather one-character length
val tokenCountsFilteredSize = df2.select(explode('words).as("word")).where(length('word) === 1).distinct

// remove terms with only one-occurrence
val rareTokens = df2.select(explode('words).as("word")).groupBy('word).count.where('count === 1).select('word)

// unioned all terms to be removed
val wholeFilters = filterNumbers.union(tokenCountsFilteredSize).union(rareTokens).distinct.cache

wholeFilters.count  // 11306

wholeFilters.printSchema
root
 |-- word: string (nullable = true)
 
val removedWords= wholeFilters.select("word").map( x => x.getString(0)).collect.toArray
removedWords: Array[String] = Array(6240, 675, 07, 296, 1436, 45670, accumulation, bv0ajl2yww, 0oahf51fti, lidfroxwzl, blakkrasta, 5g2vyfxdhb, wqmkvdzyyx, obxzwkz9us, qllmycxlyv, deweysim, w8nnq623hi, zkjjz9wxjw, sasm0nnswo, neverbiden, plovyf4x1f, t65e8s0ncr, n90qcgoakb, 4rmykq3bvb, iwj5c17o1x, qkomdvm6rl, gt4ocfptf5, k9flexsycj, d5kgln2anv, minfin, ohufz26o3n, compassionatecommunity, unworldly, 0xnbzwjdyc, gopwllhizh, idiotinchief, uev08bpmpy, rmd3xiekrq, firesell, hampsteadgardensuburb, hankallenwx, qeulj98axj, c8gd6gvz3n, pucslrfp3u, lmfmradio, efqguzv36s, u4o0f5lnkj, librarian, guldbaek, confidentiality, danyorkeshow, nidyswv2g5, r93vepyhzv, dhjieyiuh5, luthuli, 75m, vbahimh4ym, 31t3oszwdy, letbknl8su, pondlife, temporarywork, fooddemand, fbmsqev45x, jamb, enablement, wank, oscar, ...

// remove stop words
import org.apache.spark.ml.feature.StopWordsRemover
val enStopWords = StopWordsRemover.loadDefaultStopWords("english")
val remover = new StopWordsRemover().setStopWords(enStopWords).
setInputCol("words").
setOutputCol("filteredStopWords")
val df3 = remover.transform(df2)

// total words after stopwords removal
df3.select(explode('filteredStopWords).as("word")).distinct.count  // 25189

// remove tokens collected in removedListWords
import org.apache.spark.ml.feature.StopWordsRemover
val remover = new StopWordsRemover().setStopWords(removedWords).
setInputCol("filteredStopWords").
setOutputCol("filtered")
val df4 = remover.transform(df3)

// total words relevant for analysis
df4.select(explode('filtered).as("word")).distinct.count  // 13890

val dim = math.pow(2, 15).toInt  // 32768

import org.apache.spark.ml.feature.HashingTF
val tf = new HashingTF().setInputCol("filtered").
setOutputCol("TFOut").
setNumFeatures(dim)
val df5 = tf.transform(df4)

import org.apache.spark.ml.feature.IDF
val idf = new IDF().setInputCol("TFOut").setOutputCol("features")
val idfModel = idf.fit(df5)
val df6 = idfModel.transform(df5)

df6.printSchema
root
 |-- headline: string (nullable = true)
 |-- label: long (nullable = true)
 |-- words: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- filteredStopWords: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- filtered: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- TFOut: vector (nullable = true)
 |-- features: vector (nullable = true)


val Array(trainingData, testData) = df6.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
trainingData.count  // 18836

---- ML Logistic classification --------------

import org.apache.spark.ml.classification.{LogisticRegression}
val lr = new LogisticRegression
lr.setRegParam(0.01).setMaxIter(200).setFitIntercept(true)

val model = lr.fit(trainingData)
val pred = model.transform(testData).cache

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator

val bceval = new BinaryClassificationEvaluator()

bceval.evaluate(pred)  // 0.7530801473390067

val predRDD = pred.select("prediction","label").rdd.map( row => (row(0).toString.toDouble,row(1).toString.toDouble)).cache

predRDD.take(20)
res26: Array[(Double, Double)] = Array((1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (1.0,1.0), (0.0,0.0), (1.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (1.0,1.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

predRDD.filter(x => x._1 == x._2).count  // 5929
predRDD.count     // 7873
metrics.accuracy  // 0.7530801473390067

metrics.confusionMatrix
res30: org.apache.spark.mllib.linalg.Matrix =
3569.0  895.0
1049.0  2360.0

---- ML NaiveBayes classification --------------

import org.apache.spark.ml.classification.{NaiveBayes}
val nb = new NaiveBayes

val model = nb.fit(trainingData)
val pred = model.transform(testData).cache

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator

val bceval = new BinaryClassificationEvaluator()

bceval.evaluate(pred)  // 0.45855810336543346

val predRDD = pred.select("prediction","label").rdd.map( row => (row(0).toString.toDouble,row(1).toString.toDouble)).cache

predRDD.take(20)
res32: Array[(Double, Double)] = Array((1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (0.0,0.0), (1.0,0.0), (1.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (1.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,0.0), (1.0,1.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

predRDD.filter(x => x._1 == x._2).count  // 5996
predRDD.count     // 7873
metrics.accuracy  // 0.7615902451416233

metrics.confusionMatrix
res36: org.apache.spark.mllib.linalg.Matrix =
3438.0  1026.0
851.0   2558.0


