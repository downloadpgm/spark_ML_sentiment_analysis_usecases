
val reviews = spark.read.option("inferSchema","true").option("header","true").option("samplingRatio",0.05).csv("hdfs://hdpmst:9000/data/imdb_reviews.csv")

reviews.groupBy("sentiment").count.show
+---------+-----+                                                               
|sentiment|count|
+---------+-----+
|      pos|24694|
|      neg|24764|
+---------+-----+

reviews.printSchema
root
 |-- id: integer (nullable = true)
 |-- text_en: string (nullable = true)
 |-- text_pt: string (nullable = true)
 |-- sentiment: string (nullable = true)
 
val df = reviews.withColumn("label", when('sentiment === "neg",0).otherwise(1.0))

import org.apache.spark.ml.feature.RegexTokenizer
val tokenizer = new RegexTokenizer().setInputCol("text_en").setOutputCol("words").setPattern("""\W+""")

import org.apache.spark.ml.feature.SQLTransformer

val sql_explode = new SQLTransformer().setStatement("SELECT EXPLODE(words) AS word FROM __THIS__")

val regex_chknum = (word:String) => word.matches(".*\\d+.*")
spark.udf.register("regex_chknum",regex_chknum)

val sql = new SQLTransformer().setStatement("""
-- get tokens with one-character length
SELECT word FROM __THIS__ WHERE LENGTH(word) = 1   
UNION
-- remove terms with only one-occurrence
SELECT word FROM __THIS__ GROUP BY word HAVING COUNT(*) = 1  
UNION
-- filter out numbers and tokens that are words mixed with numbers
SELECT word FROM __THIS__ WHERE regex_chknum(word)   
""")

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(tokenizer,sql_explode,sql))

val model = pipeline.fit(df)

val wholeFilters = model.transform(df)

wholeFilters.count  // 48814

wholeFilters.printSchema
root
 |-- word: string (nullable = true)

val removedWords = wholeFilters.select("word").map( x => x.getString(0)).collect.toArray


// remove stop words
import org.apache.spark.ml.feature.StopWordsRemover
val enStopWords = StopWordsRemover.loadDefaultStopWords("english")
val remover = new StopWordsRemover().setStopWords(enStopWords).
setInputCol("words").
setOutputCol("filteredStopWords")

// remove tokens collected in removedListWords
import org.apache.spark.ml.feature.StopWordsRemover
val remover_sql = new StopWordsRemover().setStopWords(removedWords).
setInputCol("filteredStopWords").
setOutputCol("filtered")

val dim = math.pow(2, 16).toInt  // 131072

import org.apache.spark.ml.feature.HashingTF
val tf = new HashingTF().setInputCol("filtered").
setOutputCol("TFOut").
setNumFeatures(dim)

import org.apache.spark.ml.feature.IDF
val idf = new IDF().setInputCol("TFOut").setOutputCol("features")

import org.apache.spark.ml.classification.{LogisticRegression}
val lr = new LogisticRegression
lr.setRegParam(0.01).setMaxIter(200).setFitIntercept(true)


val pipeline1 = new Pipeline().setStages(Array(tokenizer,remover,remover_sql,tf,idf,lr))

val Array(trainingData, testData) = df.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
testData.cache

trainingData.count   // 34695

val model1 = pipeline1.fit(trainingData)

val pred = model1.transform(testData)

pred.select('label, 'prediction).show
+-----+----------+                                                              
|label|prediction|
+-----+----------+
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       1.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       1.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       1.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
+-----+----------+
only showing top 20 rows


import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")

evaluator.evaluate(pred)  // 0.8698096592833435

val predRDD = pred.select("prediction","label").rdd.map( row => (row.getDouble(0),row.getDouble(1))).cache

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

predRDD.filter(x => x._1 == x._2).count  // 12841
predRDD.count     // 14763
metrics.accuracy  // 0.8698096592833435

metrics.confusionMatrix
res11: org.apache.spark.mllib.linalg.Matrix =
6270.0  993.0
929.0   6571.0

model1.write.save("hdfs://hdpmst:9000/model/imdb")


---------------

import org.apache.spark.ml.PipelineModel
val model = PipelineModel.load("hdfs://hdpmst:9000/model/imdb")

val reviews = spark.read.option("inferSchema","true").option("header","true").option("samplingRatio",0.05).csv("hdfs://hdpmst:9000/data/imdb_reviews")

spark.conf.set("spark.sql.shuffle.partitions",10)

val df = reviews.withColumn("label", when('sentiment === "neg",0).otherwise(1.0))

val Array(trainingData, testData) = df.randomSplit(Array(0.7,0.3),11L)

val pred = model.transform(testData)

pred.select('label, 'prediction).show
+-----+----------+                                                              
|label|prediction|
+-----+----------+
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       1.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       1.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       1.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
+-----+----------+
only showing top 20 rows
