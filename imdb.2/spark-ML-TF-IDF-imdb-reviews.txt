
val rdd = sc.textFile("hdfs://hdpmst:9000/data/imdb-reviews-pt-br.csv")
val rdd1 = rdd.map( x => x.replaceAll("\"\"",""))
rdd1.saveAsTextFile("hdfs://hdpmst:9000/data/imdb_reviews.csv")

val reviews = spark.read.option("inferSchema","true").option("header","true").option("samplingRatio",0.05).csv("spark/data/imdb_br/imdb_reviews.csv")

reviews.groupBy("sentiment").count.show
+---------+-----+                                                               
|sentiment|count|
+---------+-----+
|      pos|24694|
|      neg|24764|
+---------+-----+

reviews.printSchema
root
 |-- id: integer (nullable = true)
 |-- text_en: string (nullable = true)
 |-- text_pt: string (nullable = true)
 |-- sentiment: string (nullable = true)
 
val df = reviews.withColumn("label", when('sentiment === "neg",0).otherwise(1.0))

df.printSchema
root
 |-- id: integer (nullable = true)
 |-- text_en: string (nullable = true)
 |-- text_pt: string (nullable = true)
 |-- sentiment: string (nullable = true)
 |-- label: double (nullable = false)

import org.apache.spark.ml.feature.RegexTokenizer
val tokenizer = new RegexTokenizer().setInputCol("text_en").setOutputCol("words").setPattern("""\W+""")
val df1 = tokenizer.transform(df)

val df2 = df1.select(explode('words).as("word"))

df2.cache
df2.distinct.count  // 112801

// filter out numbers and tokens that are words mixed with numbers
val filterNumbers = df2.where('word.rlike("^[0-9]*$"))

// lists tokens with one-character length
val tokenCountsFilteredSize = df2.where(length('word) === 1)

// remove terms with only one-occurrence
val rareTokens = df2.groupBy('word).count.where('count === 1).select('word)

// unioned all terms to be removed
val wholeFilters = filterNumbers.union(tokenCountsFilteredSize).union(rareTokens).distinct.cache

wholeFilters.count  // 48325

wholeFilters.printSchema
root
 |-- word: string (nullable = true)
 
val removedWords= wholeFilters.select("word").map( x => x.getString(0)).collect.toArray
removedWords: Array[String] = Array(07, 2136, 2069, crawlies, gyngell, superseded, gien, detonador, peaceniks, schaeffer, prancy, himclair, mwak, sorties, snores, tosay, rappeneau, pant, auteurist, boredomthe, mgs, udita, oland, giglio, stinkiest, 47s, technicolorian, yummmmmrodney, loons, tinos, carnegie, mendelsohn, confidentiality, nauseate, upsale, clinics, michaely, budgetmovie, vor, brandons, stilettos, tattler, manzanos, suzhou, doesif, inverting, killter, infinitesimal, mouffetard, bookshelf, fugured, goodsoup, spoilerone, condenado, phranc, obssed, llet, singling, atkinso, heinrich, aruba, dierector, invalidating, dfzs, jerkied, ferried, stotthas, soirees, paymers, forenelli, holz, ooka, trailersd, lochary, sevillo, distancing, justhopping, steelno, 57th, himor, inflammatory, r...

// remove stop words
import org.apache.spark.ml.feature.StopWordsRemover
val enStopWords = StopWordsRemover.loadDefaultStopWords("english")
val remover = new StopWordsRemover().setStopWords(enStopWords).
setInputCol("words").
setOutputCol("filteredStopWords")
val df3 = remover.transform(df1)

// total words after stopwords removal
df3.select(explode('filteredStopWords).as("word")).distinct.count  // 112670

// remove tokens collected in removedListWords
import org.apache.spark.ml.feature.StopWordsRemover
val removerOthers = new StopWordsRemover().setStopWords(removedWords).
setInputCol("filteredStopWords").
setOutputCol("filtered")
val df4 = removerOthers.transform(df3)

// total words relevant for analysis
df4.select(explode('filtered).as("word")).distinct.count  // 64349

val dim = math.pow(2, 17).toInt  // 131072

import org.apache.spark.ml.feature.HashingTF
val tf = new HashingTF().setInputCol("filtered").
setOutputCol("TFOut").
setNumFeatures(dim)
val df5 = tf.transform(df4)

import org.apache.spark.ml.feature.IDF
val idf = new IDF().setInputCol("TFOut").setOutputCol("features")
val idfModel = idf.fit(df5)
val df6 = idfModel.transform(df5)

df6.printSchema
root
 |-- id: integer (nullable = true)
 |-- text_en: string (nullable = true)
 |-- text_pt: string (nullable = true)
 |-- sentiment: string (nullable = true)
 |-- label: double (nullable = false)
 |-- words: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- filteredStopWords: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- filtered: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- TFOut: vector (nullable = true)
 |-- features: vector (nullable = true)


val Array(trainingData, testData) = df6.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
trainingData.count  // 34695

---- ML Logistic classification --------------

import org.apache.spark.ml.classification.{LogisticRegression}
val lr = new LogisticRegression
lr.setRegParam(0.01).setMaxIter(200).setFitIntercept(true)

val model = lr.fit(trainingData)
val pred = model.transform(testData)

pred.select('label, 'prediction).show
+-----+----------+                                                              
|label|prediction|
+-----+----------+
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       1.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       1.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       1.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
+-----+----------+
only showing top 20 rows


import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")

evaluator.evaluate(pred)  // 0.8696741854636592

val predRDD = pred.select("prediction","label").rdd.map( row => (row.getDouble(0),row.getDouble(1))).cache

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

predRDD.filter(x => x._1 == x._2).count  // 12839
predRDD.count     // 14763
metrics.accuracy  // 0.8696741854636592

metrics.confusionMatrix
res29: org.apache.spark.mllib.linalg.Matrix =
6270.0  993.0
931.0   6569.0


-------------------------------------


val reviews = spark.read.option("inferSchema","true").option("header","true").option("samplingRatio",0.05).csv("spark/data/imdb_br/imdb_reviews.csv")

reviews.groupBy("sentiment").count.show
+---------+-----+                                                               
|sentiment|count|
+---------+-----+
|      pos|24694|
|      neg|24764|
+---------+-----+

reviews.printSchema
root
 |-- id: integer (nullable = true)
 |-- text_en: string (nullable = true)
 |-- text_pt: string (nullable = true)
 |-- sentiment: string (nullable = true)
 
val df = reviews.withColumn("label", when('sentiment === "neg",0).otherwise(1.0))

import org.apache.spark.ml.feature.RegexTokenizer
val tokenizer = new RegexTokenizer().setInputCol("text_en").setOutputCol("words").setPattern("""\W+""")

import org.apache.spark.ml.feature.SQLTransformer

val sql_explode = new SQLTransformer().setStatement("SELECT EXPLODE(words) AS word FROM __THIS__")

val regex_chknum = (word:String) => word.matches(".*\\d+.*")
spark.udf.register("regex_chknum",regex_chknum)

val sql = new SQLTransformer().setStatement("""
-- get tokens with one-character length
SELECT word FROM __THIS__ WHERE LENGTH(word) = 1   
UNION
-- remove terms with only one-occurrence
SELECT word FROM __THIS__ GROUP BY word HAVING COUNT(*) = 1  
UNION
-- filter out numbers and tokens that are words mixed with numbers
SELECT word FROM __THIS__ WHERE regex_chknum(word)   
""")

import org.apache.spark.ml.Pipeline
val pipeline = new Pipeline().setStages(Array(tokenizer,sql_explode,sql))

val model = pipeline.fit(df)

val wholeFilters = model.transform(df)

wholeFilters.count  // 48814

wholeFilters.printSchema
root
 |-- word: string (nullable = true)

val removedWords = wholeFilters.select("word").map( x => x.getString(0)).collect.toArray


// remove stop words
import org.apache.spark.ml.feature.StopWordsRemover
val enStopWords = StopWordsRemover.loadDefaultStopWords("english")
val remover = new StopWordsRemover().setStopWords(enStopWords).
setInputCol("words").
setOutputCol("filteredStopWords")

// remove tokens collected in removedListWords
import org.apache.spark.ml.feature.StopWordsRemover
val remover_sql = new StopWordsRemover().setStopWords(removedWords).
setInputCol("filteredStopWords").
setOutputCol("filtered")

val dim = math.pow(2, 16).toInt  // 131072

import org.apache.spark.ml.feature.HashingTF
val tf = new HashingTF().setInputCol("filtered").
setOutputCol("TFOut").
setNumFeatures(dim)

import org.apache.spark.ml.feature.IDF
val idf = new IDF().setInputCol("TFOut").setOutputCol("features")

import org.apache.spark.ml.classification.{LogisticRegression}
val lr = new LogisticRegression
lr.setRegParam(0.01).setMaxIter(200).setFitIntercept(true)


val pipeline1 = new Pipeline().setStages(Array(tokenizer,remover,remover_sql,tf,idf,lr))

val Array(trainingData, testData) = df.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
testData.cache

trainingData.count   // 34695

val model1 = pipeline1.fit(trainingData)

val pred = model1.transform(testData)

pred.select('label, 'prediction).show
+-----+----------+                                                              
|label|prediction|
+-----+----------+
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       1.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       1.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       1.0|
|  0.0|       0.0|
|  0.0|       0.0|
|  0.0|       0.0|
+-----+----------+
only showing top 20 rows


import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")

evaluator.evaluate(pred)  // 0.8698096592833435

val predRDD = pred.select("prediction","label").rdd.map( row => (row.getDouble(0),row.getDouble(1))).cache

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

predRDD.filter(x => x._1 == x._2).count  // 12841
predRDD.count     // 14763
metrics.accuracy  // 0.8698096592833435

metrics.confusionMatrix
res11: org.apache.spark.mllib.linalg.Matrix =
6270.0  993.0
929.0   6571.0