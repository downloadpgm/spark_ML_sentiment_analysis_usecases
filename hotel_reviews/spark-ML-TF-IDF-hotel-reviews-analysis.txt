val df = spark.read.format("csv").option("header","true").option("inferSchema","true").option("quoteAll","true").load("hotel_reviews/tripadvisor_hotel_reviews.csv")

df.printSchema
root
 |-- Review: string (nullable = true)
 |-- Rating: integer (nullable = true)
 
df.select("Rating").distinct.show
+------+
|Rating|
+------+
|     1|
|     3|
|     5|
|     4|
|     2|
+------+

val df1 = df.withColumnRenamed("Rating", "label")

import org.apache.spark.ml.feature.RegexTokenizer
val tokenizer = new RegexTokenizer().setInputCol("Review").setOutputCol("words").setPattern("""\W+""")
val df2 = tokenizer.transform(df1)

val filterNumbers = df2.select(explode('words).as("word")).where('word.cast("int").isNotNull).distinct

val tokenCountsFilteredSize = df2.select(explode('words).as("word")).where(length('word) === 1).distinct

val rareTokens = df2.select(explode('words).as("word")).groupBy('word).count.where('count === 1).select('word)

val wholeFilters = filterNumbers.union(tokenCountsFilteredSize).union(rareTokens).distinct

wholeFilters.printSchema
root
 |-- word: string (nullable = true)
 
val removedTokens = wholeFilters.agg(collect_list('word).as("removedwords"))

removedTokens.show
+--------------------+
|        removedwords|
+--------------------+
|[07, 296, 4032, 6...|
+--------------------+

val removedWords = removedTokens.rdd.map( row => row(0)).map( x => x.toString )

removedWords.take(1)
res24: Array[String] = Array(WrappedArray(07, 296, 4032, 675, 1512, inwas, telephonecommunication, uglyiest, comping, throhg, bookshelf, 57th, invisable, buba, tylers, wharfe, peter__, rtitz, peolpe, 10sep, honeymoom, balding, trotts, boking, gastronomical, catamara, appproached, seralles, hooney, prisms, geoge, reasoable, smarties, spendaholic, aquos, onaccessible, matter__, davidsan, smorgy, smokefree, feltwe, michelet, seating__, melodic, effer, soirees, recoment, tarnish, clubmany, evening4, reconditionning, cubs, mng, bateaux, ransom, consistenly, terino, trobadero, tuchinski, intercontis, nicotine, ascharming, rejuvenate, spotaround, isssus, prblems, bombonera, escalotor, procrastinators, 20au, hse, inverted, bricked, junp, laterwatch, torontonian, 75m, seperation, queiter, hereev...

val removedListWords = removedWords.collect
removedListWords: Array[String] = Array(WrappedArray(07, 296, 4032, 675, 1512, inwas, telephonecommunication, uglyiest, comping, throhg, bookshelf, 57th, invisable, buba, tylers, wharfe, peter__, rtitz, peolpe, 10sep, honeymoom, balding, trotts, boking, gastronomical, catamara, appproached, seralles, hooney, prisms, geoge, reasoable, smarties, spendaholic, aquos, onaccessible, matter__, davidsan, smorgy, smokefree, feltwe, michelet, seating__, melodic, effer, soirees, recoment, tarnish, clubmany, evening4, reconditionning, cubs, mng, bateaux, ransom, consistenly, terino, trobadero, tuchinski, intercontis, nicotine, ascharming, rejuvenate, spotaround, isssus, prblems, bombonera, escalotor, procrastinators, 20au, hse, inverted, bricked, junp, laterwatch, torontonian, 75m, seperation, quei...


import org.apache.spark.ml.feature.StopWordsRemover
val remover = new StopWordsRemover().setStopWords(removedListWords).
setInputCol("words").
setOutputCol("filteredRared")
val df3 = remover.transform(df2)


val enStopWords = StopWordsRemover.loadDefaultStopWords("english")
val remover = new StopWordsRemover().setStopWords(enStopWords).
setInputCol("filteredRared").
setOutputCol("filtered")
val df4 = remover.transform(df3)

df4.select(max(size('filtered)).as("max_size")).show
+--------+
|max_size|
+--------+
|    1923|
+--------+

import org.apache.spark.ml.feature.HashingTF
val tf = new HashingTF().setInputCol("filtered").
setOutputCol("TFOut").
setNumFeatures(dim)
val df5 = tf.transform(df4)

import org.apache.spark.ml.feature.IDF
val idf = new IDF().setInputCol("TFOut").setOutputCol("features")
val idfModel = idf.fit(df5)
val df6 = idfModel.transform(df5)

import org.apache.spark.ml.feature.{StringIndexer, OneHotEncoder, VectorAssembler}

import org.apache.spark.ml.classification.{LogisticRegression, OneVsRest}
val lr = new LogisticRegression
lr.setRegParam(0.01).setMaxIter(200).setFitIntercept(true)

val ovr = new OneVsRest().setClassifier(lr)

val Array(trainingData, testData) = df6.randomSplit(Array(0.7,0.3),11L)

trainingData.cache

trainingData.count
res29: Long = 14468

val ovrmodel = ovr.fit(trainingData)

val pred = ovrmodel.transform(testData)

import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")

val accuracy = evaluator.evaluate(pred)
accuracy: Double = 0.0

val predRDD = pred.select("prediction","label").rdd.map( row => (row(0).toString.toDouble,row(1).toString.toDouble)).cache

predRDD.take(2)res12: Array[(Double, Double)] = Array((0.0,3.0), (0.0,0.0))                    

import org.apache.spark.mllib.evaluation.MulticlassMetrics

val metrics = new MulticlassMetrics(predRDD)

predRDD.filter(x => x._1 == x._2).count
res13: Long = 0                                                           

predRDD.count
res14: Long = 6023

metrics.accuracy
res34: Double = 0.0

metrics.confusionMatrix
res16: org.apache.spark.mllib.linalg.Matrix =
31642.0  69.0    61.0    0.0  0.0  0.0  0.0
1541.0   9176.0  824.0   0.0  0.0  0.0  0.0
654.0    967.0   6585.0  0.0  0.0  0.0  0.0
5090.0   1.0     1.0     0.0  0.0  0.0  0.0
1808.0   74.0    1.0     0.0  0.0  0.0  0.0
531.0    6.0     2.0     0.0  0.0  0.0  0.0
20.0     0.0     0.0     0.0  0.0  0.0  0.0


