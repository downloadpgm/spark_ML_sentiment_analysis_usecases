
val df = spark.read.format("csv").option("header","true").option("inferSchema","true").option("quoteAll","true").load("hotel_reviews/tripadvisor_hotel_reviews.csv")

df.printSchema
root
 |-- Review: string (nullable = true)
 |-- Rating: integer (nullable = true)
 
df.select("Rating").distinct.show
+------+
|Rating|
+------+
|     1|
|     3|
|     5|
|     4|
|     2|
+------+

import org.apache.spark.sql.types._
val df1 = df.withColumn("label", (col("Rating")-1).cast(DoubleType))

df1.show
+--------------------+------+-----+
|              Review|Rating|label|
+--------------------+------+-----+
|nice hotel expens...|     4|  3.0|
|ok nothing specia...|     2|  1.0|
|nice rooms not 4*...|     3|  2.0|
|unique, great sta...|     5|  4.0|
|great stay great ...|     5|  4.0|
|love monaco staff...|     5|  4.0|
|cozy stay rainy c...|     5|  4.0|
|excellent staff, ...|     4|  3.0|
|hotel stayed hote...|     5|  4.0|
|excellent stayed ...|     5|  4.0|
|poor value stayed...|     2|  1.0|
|nice value seattl...|     4|  3.0|
|nice hotel good l...|     4|  3.0|
|nice hotel not ni...|     3|  2.0|
|great hotel night...|     4|  3.0|
|horrible customer...|     1|  0.0|
|disappointed say ...|     2|  1.0|
|fantastic stay mo...|     5|  4.0|
|good choice hotel...|     5|  4.0|
|hmmmmm say really...|     3|  2.0|
+--------------------+------+-----+
only showing top 20 rows

df1.printSchema
root
 |-- Review: string (nullable = true)
 |-- Rating: integer (nullable = true)
 |-- label: double (nullable = true)

import org.apache.spark.ml.feature.RegexTokenizer
val tokenizer = new RegexTokenizer().setInputCol("Review").setOutputCol("words").setPattern("""\W+""")
val df2 = tokenizer.transform(df1)

df2.select(explode('words).as("word")).distinct.count
res3: Long = 52715

// filter out numbers and tokens that are words mixed with numbers
val filterNumbers = df2.select(explode('words).as("word")).where('word.rlike("^[0-9]*$")).distinct

// lists tokens greather one-character length
val tokenCountsFilteredSize = df2.select(explode('words).as("word")).where(length('word) === 1).distinct

// remove terms with only one-occurrence
val rareTokens = df2.select(explode('words).as("word")).groupBy('word).count.where('count === 1).select('word)

// unioned all terms to be removed
val wholeFilters = filterNumbers.union(tokenCountsFilteredSize).union(rareTokens).distinct.cache

wholeFilters.count
res4: Long = 26861

wholeFilters.printSchema
root
 |-- word: string (nullable = true)
 
val removedWords= wholeFilters.select("word").map( x => x.getString(0)).collect.toArray
removedWords: Array[String] = Array(2240876, 07, 2235212, 2222814, 2229610, 2293424, 2200840, 2288878, 2200535, 296, 2081310, 2104547, 1955583, 1734557, 1966363, 1970480, 1965893, 1632896, 1596252, 1442175, 1528293, 1588635, 1609594, 1591770, 1937610, 1482904, 1575456, 1527737, 1425137, 1632183, 1516351, 1631447, 1441756, 1424699, 1515207, 1441956, 1527418, 1385384, 1424210, 3127468040, 1322530, 1396135, 1277616, 675, 31106617, 1227875, 1098228, 1154694, 1229790, 525313, 580480, 156277, 164418, 250487, 428783, 63814, 80305, deleterious, maximun, 540lux, laterel, unfinsished, responsibiility, alergent, docmentation, filterts, involving, orvetto, painters, zarlengo, removee, icediinstructed, fog, residoe, ctritical, certifiedmanager, blossom, reorganizeed, asmooth, underneatht, harder, ea...

// remove stop words
import org.apache.spark.ml.feature.StopWordsRemover
val enStopWords = StopWordsRemover.loadDefaultStopWords("english")
val remover = new StopWordsRemover().setStopWords(enStopWords).
setInputCol("words").
setOutputCol("filteredStopWords")
val df3 = remover.transform(df2)

// total words after stopwords removal
df3.select(explode('filteredStopWords).as("word")).distinct.count
res8: Long = 52586

// remove tokens collected in removedListWords
import org.apache.spark.ml.feature.StopWordsRemover
val remover = new StopWordsRemover().setStopWords(removedWords).
setInputCol("filteredStopWords").
setOutputCol("filtered")
val df4 = remover.transform(df3)

// total words relevant for analysis
df4.select(explode('filtered).as("word")).distinct.count
res9: Long = 25733

val dim = math.pow(2, 15).toInt
dim: Int = 32768

import org.apache.spark.ml.feature.HashingTF
val tf = new HashingTF().setInputCol("filtered").
setOutputCol("TFOut").
setNumFeatures(dim)
val df5 = tf.transform(df4)

import org.apache.spark.ml.feature.IDF
val idf = new IDF().setInputCol("TFOut").setOutputCol("features")
val idfModel = idf.fit(df5)
val df6 = idfModel.transform(df5)

df6.printSchema
root
 |-- Review: string (nullable = true)
 |-- Rating: integer (nullable = true)
 |-- label: double (nullable = true)
 |-- words: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- filteredStopWords: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- filtered: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- TFOut: vector (nullable = true)
 |-- features: vector (nullable = true)


import org.apache.spark.ml.classification.{LogisticRegression, OneVsRest}
val lr = new LogisticRegression
lr.setRegParam(0.01).setMaxIter(200).setFitIntercept(true)

val ovr = new OneVsRest().setClassifier(lr)

val Array(trainingData, testData) = df6.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
trainingData.count  // 14468

---- ML OneVsRest classification --------------

val ovrmodel = ovr.fit(trainingData)
val pred = ovrmodel.transform(testData).cache

import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")

evaluator.evaluate(pred)  // 0.5183463390337041

val predRDD = pred.select("prediction","label").rdd.map( row => (row(0).toString.toDouble,row(1).toString.toDouble)).cache

predRDD.take(20)
res30: Array[(Double, Double)] = Array((1.0,1.0), (2.0,0.0), (1.0,3.0), (4.0,4.0), (4.0,4.0), (4.0,4.0), (4.0,3.0), (3.0,4.0), (3.0,3.0), (0.0,1.0), (4.0,4.0), (3.0,1.0), (4.0,4.0), (4.0,1.0), (4.0,1.0), (3.0,1.0), (3.0,4.0), (4.0,4.0), (2.0,1.0), (4.0,4.0))
import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

predRDD.filter(x => x._1 == x._2).count  // 3122
predRDD.count     // 6023
metrics.accuracy  // 0.5183463390337041

metrics.confusionMatrix
res34: org.apache.spark.mllib.linalg.Matrix =
178.0  111.0  52.0   50.0   46.0
71.0   129.0  120.0  139.0  91.0
31.0   64.0   130.0  271.0  139.0
12.0   47.0   116.0  763.0  805.0
4.0    15.0   48.0   669.0  1922.0

---- ML Naive Bayes classification --------------

import org.apache.spark.ml.classification.NaiveBayes
val model = new NaiveBayes().fit(trainingData)

val pred = model.transform(testData).cache

import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")

evaluator.evaluate(pred)  // 0.20786983230948033

val predRDD = pred.select("prediction","label").rdd.map( row => (row(0).toString.toDouble,row(1).toString.toDouble)).cache

predRDD.take(20)
res36: Array[(Double, Double)] = Array((1.0,1.0), (0.0,0.0), (2.0,3.0), (4.0,4.0), (4.0,4.0), (1.0,4.0), (2.0,3.0), (3.0,4.0), (3.0,3.0), (1.0,1.0), (4.0,4.0), (0.0,1.0), (4.0,4.0), (1.0,1.0), (4.0,1.0), (0.0,1.0), (4.0,4.0), (4.0,4.0), (2.0,1.0), (4.0,4.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

predRDD.filter(x => x._1 == x._2).count  // 3225
predRDD.count  // 6023
metrics.accuracy  // 0.5354474514361613

metrics.confusionMatrix
res40: org.apache.spark.mllib.linalg.Matrix =
223.0  134.0  46.0   19.0   15.0
107.0  189.0  135.0  90.0   29.0
39.0   97.0   148.0  243.0  108.0
14.0   62.0   172.0  755.0  740.0
10.0   30.0   71.0   637.0  1910.0

