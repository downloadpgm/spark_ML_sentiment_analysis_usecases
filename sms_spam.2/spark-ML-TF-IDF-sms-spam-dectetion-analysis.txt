val df = spark.read.format("csv").option("header","true").option("inferSchema","true").option("quoteAll","true").load("spark/data/spam/sms_spam.csv")
df: org.apache.spark.sql.DataFrame = [v1: string, v2: string ... 3 more fields]

scala> df.printSchema
root
 |-- v1: string (nullable = true)
 |-- v2: string (nullable = true)
 |-- _c2: string (nullable = true)
 |-- _c3: string (nullable = true)
 |-- _c4: string (nullable = true)

df.show
22/03/28 17:45:19 WARN CSVDataSource: CSV header does not conform to the schema.
 Header: v1, v2, , ,
 Schema: v1, v2, _c2, _c3, _c4
Expected: _c2 but found:
CSV file: file:///u01/app/oracle/spark/data/spam/sms_spam.csv
+----+--------------------+----+----+----+
|  v1|                  v2| _c2| _c3| _c4|
+----+--------------------+----+----+----+
| ham|Go until jurong p...|null|null|null|
| ham|Ok lar... Joking ...|null|null|null|
|spam|Free entry in 2 a...|null|null|null|
| ham|U dun say so earl...|null|null|null|
| ham|Nah I don't think...|null|null|null|
|spam|FreeMsg Hey there...|null|null|null|
| ham|Even my brother i...|null|null|null|
| ham|As per your reque...|null|null|null|
|spam|WINNER!! As a val...|null|null|null|
|spam|Had your mobile 1...|null|null|null|
| ham|I'm gonna be home...|null|null|null|
|spam|SIX chances to wi...|null|null|null|
|spam|URGENT! You have ...|null|null|null|
| ham|I've been searchi...|null|null|null|
| ham|I HAVE A DATE ON ...|null|null|null|
|spam|XXXMobileMovieClu...|null|null|null|
| ham|Oh k...i'm watchi...|null|null|null|
| ham|Eh u remember how...|null|null|null|
| ham|Fine if that��s t...|null|null|null|
|spam|England v Macedon...|null|null|null|
+----+--------------------+----+----+----+
only showing top 20 rows

val df1 = df.where("v1 is not null and v2 is not null").select("v1","v2")

df1.printSchema
root
 |-- v1: string (nullable = true)
 |-- v2: string (nullable = true)

import org.apache.spark.ml.feature.RegexTokenizer
val tokenizer = new RegexTokenizer().setInputCol("v2").setOutputCol("words").setPattern("""\W+""")
val df2 = tokenizer.transform(df1)

df2.select(explode('words).as("word")).distinct.count  // 8662

// filter out numbers and tokens that are words mixed with numbers
val filterNumbers = df2.select(explode('words).as("word")).where('word.rlike("^[0-9]*$")).distinct

// lists tokens greather one-character length
val tokenCountsFilteredSize = df2.select(explode('words).as("word")).where(length('word) === 1).distinct

// remove terms with only one-occurrence
val rareTokens = df2.select(explode('words).as("word")).groupBy('word).count.where('count === 1).select('word)

// unioned all terms to be removed
val wholeFilters = filterNumbers.union(tokenCountsFilteredSize).union(rareTokens).distinct.cache

wholeFilters.count  // 4638

wholeFilters.printSchema
root
 |-- word: string (nullable = true)
 
val removedWords= wholeFilters.select("word").map( x => x.getString(0)).collect.toArray
removedWords: Array[String] = Array(89555, 07, 09065989182, 9280114, bookshelf, spoil, 3x, okies, cnupdates, gua, inner, arguments, boyf, rummer, outfit, traveling, priority, creativity, accumulation, connected, oscar, 087147123779am, harder, fiting, jewelry, mgs, 08712300220, 3650, 800, 08707808226, 125, aig, vibrate, marley, atten, weirdo, parts, tescos, brilliantly, outgoing, 1mega, highest, otbox, beverage, craigslist, pocay, positive, wahleykkum, surrounded, arguing, 7am, hourish, evaluation, charts, ld, lark, 1er, directors, achieve, taxt, scarcasim, impressively, der, fletcher, yaxx, studentfinancial, greece, cable, breezy, haunt, oblivious, undrstnd, australia, disaster, travelled, clover, afterwards, nighters, 7, 81151, 078498, 08718726270, dawns, werethe, prevent, gymnastics, ...

// remove stop words
import org.apache.spark.ml.feature.StopWordsRemover
val enStopWords = StopWordsRemover.loadDefaultStopWords("english")
val remover = new StopWordsRemover().setStopWords(enStopWords).
setInputCol("words").
setOutputCol("filteredStopWords")
val df3 = remover.transform(df2)

// total words after stopwords removal
df3.select(explode('filteredStopWords).as("word")).distinct.count  // 8537

// remove tokens collected in removedListWords
import org.apache.spark.ml.feature.StopWordsRemover
val remover = new StopWordsRemover().setStopWords(removedWords).
setInputCol("filteredStopWords").
setOutputCol("filtered")
val df4 = remover.transform(df3)

// total words relevant for analysis
df4.select(explode('filtered).as("word")).distinct.count  // 3909

val dim = math.pow(2, 13).toInt  // 8192

import org.apache.spark.ml.feature.HashingTF
val tf = new HashingTF().setInputCol("filtered").
setOutputCol("TFOut").
setNumFeatures(dim)
val df5 = tf.transform(df4)

import org.apache.spark.ml.feature.IDF
val idf = new IDF().setInputCol("TFOut").setOutputCol("features")
val idfModel = idf.fit(df5)
val df6 = idfModel.transform(df5)

df6.printSchema
root
 |-- v1: string (nullable = true)
 |-- v2: string (nullable = true)
 |-- words: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- filteredStopWords: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- filtered: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- TFOut: vector (nullable = true)
 |-- features: vector (nullable = true)


import org.apache.spark.ml.feature.{StringIndexer}

val resultStrIdx = new StringIndexer().setInputCol("v1").setOutputCol("label")
val df7 = resultStrIdx.fit(df6).transform(df6)

val Array(trainingData, testData) = df7.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
trainingData.count  // 3974

---- ML Logistic classification --------------

import org.apache.spark.ml.classification.{LogisticRegression}
val lr = new LogisticRegression
lr.setRegParam(0.01).setMaxIter(200).setFitIntercept(true)

val model = lr.fit(trainingData)
val pred = model.transform(testData).cache

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator

val bceval = new BinaryClassificationEvaluator()

bceval.evaluate(pred)  // 0.98860432797302

val predRDD = pred.select("prediction","label").rdd.map( row => (row(0).toString.toDouble,row(1).toString.toDouble)).cache

predRDD.take(20)
res17: Array[(Double, Double)] = Array((0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

predRDD.filter(x => x._1 == x._2).count  // 1566
predRDD.count     // 1599
metrics.accuracy  // 0.9793621013133208

metrics.confusionMatrix
res21: org.apache.spark.mllib.linalg.Matrix =
1378.0  2.0
31.0    188.0

---- ML NaiveBayes classification --------------

import org.apache.spark.ml.classification.{NaiveBayes}
val nb = new NaiveBayes

val model = nb.fit(trainingData)
val pred = model.transform(testData).cache

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator

val bceval = new BinaryClassificationEvaluator()

bceval.evaluate(pred)  // 0.22573952749652565

val predRDD = pred.select("prediction","label").rdd.map( row => (row(0).toString.toDouble,row(1).toString.toDouble)).cache

predRDD.take(20)
res23: Array[(Double, Double)] = Array((0.0,0.0), (1.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

predRDD.filter(x => x._1 == x._2).count  // 1548
predRDD.count     // 1599
metrics.accuracy  // 0.9681050656660413

metrics.confusionMatrix
res27: org.apache.spark.mllib.linalg.Matrix =
1345.0  35.0
16.0    203.0
