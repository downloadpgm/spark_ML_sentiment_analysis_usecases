
val df = spark.read.format("csv").option("header","true").option("quoteAll","true").load("food_inspections/food-inspections.csv")

df.printSchema
root
 |-- Inspection ID: string (nullable = true)
 |-- DBA Name: string (nullable = true)
 |-- AKA Name: string (nullable = true)
 |-- License #: string (nullable = true)
 |-- Facility Type: string (nullable = true)
 |-- Risk: string (nullable = true)
 |-- Address: string (nullable = true)
 |-- City: string (nullable = true)
 |-- State: string (nullable = true)
 |-- Zip: string (nullable = true)
 |-- Inspection Date: string (nullable = true)
 |-- Inspection Type: string (nullable = true)
 |-- Results: string (nullable = true)
 |-- Violations: string (nullable = true)
 |-- Latitude: string (nullable = true)
 |-- Longitude: string (nullable = true)
 |-- Location: string (nullable = true)
 |-- Historical Wards 2003-2015: string (nullable = true)
 |-- Zip Codes: string (nullable = true)
 |-- Community Areas: string (nullable = true)
 |-- Census Tracts: string (nullable = true)
 |-- Wards: string (nullable = true)
 
df.groupBy("Results").count.show
+--------------------+------+
|             Results| count|
+--------------------+------+
|           Not Ready|  1912|
|                Fail| 38087|
|            No Entry|  6324|
|Business Not Located|    69|
|  Pass w/ Conditions| 27448|
|     Out of Business| 16919|
|                Pass|106066|
+--------------------+------+

df.select("Violations","Results").show
+--------------------+------------------+
|          Violations|           Results|
+--------------------+------------------+
|3. MANAGEMENT, FO...|Pass w/ Conditions|
|10. ADEQUATE HAND...|              Pass|
|                null|         Not Ready|
|3. MANAGEMENT, FO...|Pass w/ Conditions|
|                null|              Pass|
|                null|              Pass|
|                null|Pass w/ Conditions|
|5. PROCEDURES FOR...|Pass w/ Conditions|
|49. NON-FOOD/FOOD...|              Pass|
|                null|              Pass|
|25. CONSUMER ADVI...|Pass w/ Conditions|
|                null|         Not Ready|
|                null|Pass w/ Conditions|
|                null|              Fail|
|3. MANAGEMENT, FO...|Pass w/ Conditions|
|45. SINGLE-USE/SI...|              Pass|
|                null|              Pass|
|                null|              Pass|
|3. MANAGEMENT, FO...|Pass w/ Conditions|
|                null|         Not Ready|
+--------------------+------------------+
only showing top 20 rows


val df1 = df.select('Results, lower('Violations).as("Violations")).na.fill("NA", Array("Violations"))

import org.apache.spark.ml.feature.RegexTokenizer
val tokenizer = new RegexTokenizer().setInputCol("Violations").setOutputCol("words").setPattern("""\W+""")
val df2 = tokenizer.transform(df1)

df2.select(explode('words).as("word")).distinct.count
res2: Long = 46008

// filter out numbers and tokens that are words mixed with numbers
val filterNumbers = df2.select(explode('words).as("word")).where('word.rlike("^[0-9]*$")).distinct

// lists tokens greather one-character length
val tokenCountsFilteredSize = df2.select(explode('words).as("word")).where(length('word) === 1).distinct

// remove terms with only one-occurrence
val rareTokens = df2.select(explode('words).as("word")).groupBy('word).count.where('count === 1).select('word)

// unioned all terms to be removed
val wholeFilters = filterNumbers.union(tokenCountsFilteredSize).union(rareTokens).distinct.cache

wholeFilters.count
res0: Long = 26229

wholeFilters.printSchema
root
 |-- word: string (nullable = true)
 
val removedWords = wholeFilters.select("word").map( x => x.getString(0)).collect.toArray
removedWords: Array[String] = Array(2240876, 07, 2235212, 2222814, 2229610, 2293424, 2200840, 2288878, 2200535, 296, 2081310, 2104547, 1955583, 1734557, 1966363, 1970480, 1965893, 1632896, 1596252, 1442175, 1528293, 1588635, 1609594, 1591770, 1937610, 1482904, 1575456, 1527737, 1425137, 1632183, 1516351, 1631447, 1441756, 1424699, 1515207, 1441956, 1527418, 1385384, 1424210, 3127468040, 1322530, 1396135, 1277616, 675, 31106617, 1227875, 1098228, 1154694, 1229790, 525313, 580480, 156277, 164418, 250487, 428783, 63814, 80305, deleterious, maximun, 540lux, laterel, unfinsished, responsibiility, alergent, docmentation, filterts, involving, orvetto, painters, zarlengo, removee, icediinstructed, fog, residoe, ctritical, certifiedmanager, blossom, reorganizeed, asmooth, underneatht, harder, ea...

// remove stop words
import org.apache.spark.ml.feature.StopWordsRemover
val enStopWords = StopWordsRemover.loadDefaultStopWords("english")
val remover = new StopWordsRemover().setStopWords(enStopWords).
setInputCol("words").
setOutputCol("filteredStopWords")
val df3 = remover.transform(df2)

// total words after stopwords removal
df3.select(explode('filteredStopWords).as("word")).distinct.count
res49: Long = 45883

// remove tokens collected in removedListWords
import org.apache.spark.ml.feature.StopWordsRemover
val remover = new StopWordsRemover().setStopWords(removedWords).
setInputCol("filteredStopWords").
setOutputCol("filtered")
val df4 = remover.transform(df3)

// total words relevant for analysis
df4.select(explode('filtered).as("word")).distinct.count
res38: Long = 19660

val dim = math.pow(2, 15).toInt
dim: Int = 32768

import org.apache.spark.ml.feature.HashingTF
val tf = new HashingTF().setInputCol("filtered").
setOutputCol("TFOut").
setNumFeatures(dim)
val df5 = tf.transform(df4)

import org.apache.spark.ml.feature.IDF
val idf = new IDF().setInputCol("TFOut").setOutputCol("features")
val idfModel = idf.fit(df5)
val df6 = idfModel.transform(df5)

df6.printSchema
root
 |-- Results: string (nullable = true)
 |-- Violations: string (nullable = false)
 |-- words: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- filteredStopWords: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- filtered: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- TFOut: vector (nullable = true)
 |-- features: vector (nullable = true)


import org.apache.spark.ml.feature.{StringIndexer}

val resultStrIdx = new StringIndexer().setInputCol("Results").setOutputCol("label")
val df7 = resultStrIdx.fit(df6).transform(df6)

val Array(trainingData, testData) = df7.randomSplit(Array(0.7,0.3),11L)

trainingData.cache
trainingData.count  // 137772

---- ML OneVsRest classification --------------

import org.apache.spark.ml.classification.{LogisticRegression, OneVsRest}
val lr = new LogisticRegression
lr.setRegParam(0.01).setMaxIter(200).setFitIntercept(true)

val ovr = new OneVsRest().setClassifier(lr)

val ovrmodel = ovr.fit(trainingData)
val pred = ovrmodel.transform(testData).cache

import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")

evaluator.evaluate(pred)  // 0.7959460145970568

val predRDD = pred.select("prediction","label").rdd.map( row => (row(0).toString.toDouble,row(1).toString.toDouble)).cache

predRDD.take(20)
res8: Array[(Double, Double)] = Array((0.0,6.0), (0.0,6.0), (0.0,6.0), (0.0,6.0), (0.0,6.0), (0.0,6.0), (2.0,1.0), (1.0,1.0), (1.0,1.0), (2.0,1.0), (1.0,1.0), (1.0,1.0), (1.0,1.0), (2.0,1.0), (1.0,1.0), (1.0,1.0), (2.0,1.0), (2.0,1.0), (1.0,1.0), (1.0,1.0))

import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

predRDD.filter(x => x._1 == x._2).count  // 47003
predRDD.count     // 59053
metrics.accuracy  // 0.7959460145970568

metrics.confusionMatrix
res12: org.apache.spark.mllib.linalg.Matrix =
31646.0  110.0   108.0   0.0  6.0   0.0  0.0
1483.0   8810.0  933.0   0.0  14.0  2.0  0.0
643.0    1142.0  6546.0  0.0  3.0   1.0  0.0
5103.0   5.0     0.0     0.0  0.0   0.0  0.0
1831.0   61.0    6.0     0.0  1.0   0.0  0.0
566.0    3.0     2.0     0.0  0.0   0.0  0.0
28.0     0.0     0.0     0.0  0.0   0.0  0.0

---- ML Naive Bayes classification --------------

import org.apache.spark.ml.classification.NaiveBayes
val model = new NaiveBayes().fit(trainingData)

val pred = model.transform(testData).cache

import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")

evaluator.evaluate(pred)  // 0.7161024842091003

val predRDD = pred.select("prediction","label").rdd.map( row => (row(0).toString.toDouble,row(1).toString.toDouble)).cache

predRDD.take(20)
res6: Array[(Double, Double)] = Array((3.0,6.0), (3.0,6.0), (3.0,6.0), (3.0,6.0), (3.0,6.0), (3.0,6.0), (2.0,1.0), (2.0,1.0), (2.0,1.0), (2.0,1.0), (1.0,1.0), (2.0,1.0), (2.0,1.0), (2.0,1.0), (1.0,1.0), (2.0,1.0), (2.0,1.0), (2.0,1.0), (1.0,1.0), (2.0,1.0))
import org.apache.spark.mllib.evaluation.MulticlassMetrics
val metrics = new MulticlassMetrics(predRDD)

predRDD.filter(x => x._1 == x._2).count  // 42288
predRDD.count     // 59053
metrics.accuracy  // 0.7161024842091003

metrics.confusionMatrix
res10: org.apache.spark.mllib.linalg.Matrix =                                   
22937.0  700.0   783.0   7234.0  209.0  7.0  0.0
772.0    7858.0  1597.0  914.0   97.0   4.0  0.0
303.0    1477.0  6384.0  141.0   28.0   2.0  0.0
2.0      5.0     0.0     5101.0  0.0    0.0  0.0
49.0     55.0    16.0    1771.0  8.0    0.0  0.0
6.0      3.0     3.0     559.0   0.0    0.0  0.0
0.0      0.0     0.0     28.0    0.0    0.0  0.0